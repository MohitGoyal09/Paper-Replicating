{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGVgfSK6eyy4"
      },
      "outputs": [],
      "source": [
        "# contains total 8 layers with 5 convs and 3 dense\n",
        "# input images size = 256 x 256\n",
        "# softmax activation function after last layer\n",
        "# Response Normalization layers apply after only to 1st and 2nd Convs layer\n",
        "# follow MaxPolling layer after Response Normalization layers and also after the 5th Conv layer\n",
        "# apply ReLU after every MaxPooling layer and fully connected layer\n",
        "\n",
        "# -- CNN layers --\n",
        "# 1st layer -> input_dim = 224 x 224 x 3, output_dim = 11 x 11 x 3, kernels = 96, stride = 4\n",
        "# 2nd layer -> input_dim = 11 x 11 x 3, output_dim = 5 x 5 x 48, kernels = 256, stride = 1\n",
        "# 3rd layer -> input_dim = 5 x 5 x 48, output_dim = 3 x 3 x 256, kernels = 348, stride = 1\n",
        "# 4th layer -> input_dim = 3 x 3 x 256, output_dim = 3 x 3 x 192, kenel_size = 348, stride = 1\n",
        "# 5th layer -> input_dim = 3 x 3 x 192, output_dim = 3 x 3 x 192, kernels = 256, stride = 1\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, no_of_classes):\n",
        "        super(AlexNet, self).__init__()\n",
        "\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=1),  # N x 96 x 55 x 55,\n",
        "            nn.LocalResponseNorm(size=5),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), # N x 96 x 27 x 27,\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5),  # N x 256 x 23 x 23\n",
        "            nn.LocalResponseNorm(size=5),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # N x 256 x 11 x 11\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=256, out_channels=348, kernel_size=3),  # N x 348 x 9 x 9\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=348, out_channels=348, kernel_size=3), # N x 348 x 7 x 7\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=348, out_channels=256, kernel_size=3),  # N x 348 x 5 x 5\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), # N x 256 x 2 x 2\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(), # N x 1024\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=(256 * 2 * 2), out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "        self.init_parameter()\n",
        "\n",
        "    def init_parameter(self):\n",
        "        for layer in self.convs:\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.normal_(layer.weight, mean=0, std=0.1),\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "        nn.init.constant_(self.convs[4].bias, 1)\n",
        "        nn.init.constant_(self.convs[10].bias, 1)\n",
        "        nn.init.constant_(self.convs[12].bias, 1)\n",
        "        nn.init.constant_(self.classifier[2].bias, 1)\n",
        "        nn.init.constant_(self.classifier[5].bias, 1)\n",
        "        nn.init.constant_(self.classifier[7].bias, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    }
  ]
}